
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import cross_val_score
df = pd.read_csv("dataset/train.csv")
df.head()
Loan_ID	Gender	Married	Dependents	Education	Self_Employed	ApplicantIncome	CoapplicantIncome	LoanAmount	Loan_Amount_Term	Credit_History	Property_Area	Loan_Status
0	LP001002	Male	No	0	Graduate	No	5849	0.0	NaN	360.0	1.0	Urban	Y
1	LP001003	Male	Yes	1	Graduate	No	4583	1508.0	128.0	360.0	1.0	Rural	N
2	LP001005	Male	Yes	0	Graduate	Yes	3000	0.0	66.0	360.0	1.0	Urban	Y
3	LP001006	Male	Yes	0	Not Graduate	No	2583	2358.0	120.0	360.0	1.0	Urban	Y
4	LP001008	Male	No	0	Graduate	No	6000	0.0	141.0	360.0	1.0	Urban	Y
# Drop the Loan_ID column as it is of no use for model.
data = df.drop(columns=["Loan_ID"])
data.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 614 entries, 0 to 613
Data columns (total 12 columns):
 #   Column             Non-Null Count  Dtype  
---  ------             --------------  -----  
 0   Gender             601 non-null    object 
 1   Married            611 non-null    object 
 2   Dependents         599 non-null    object 
 3   Education          614 non-null    object 
 4   Self_Employed      582 non-null    object 
 5   ApplicantIncome    614 non-null    int64  
 6   CoapplicantIncome  614 non-null    float64
 7   LoanAmount         592 non-null    float64
 8   Loan_Amount_Term   600 non-null    float64
 9   Credit_History     564 non-null    float64
 10  Property_Area      614 non-null    object 
 11  Loan_Status        614 non-null    object 
dtypes: float64(4), int64(1), object(7)
memory usage: 57.7+ KB
data.columns
Index(['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed',
       'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',
       'Loan_Amount_Term', 'Credit_History', 'Property_Area', 'Loan_Status'],
      dtype='object')
categorical_data = [i for i in data.columns if data[i].dtype=="object"]
categorical_data
['Gender',
 'Married',
 'Dependents',
 'Education',
 'Self_Employed',
 'Property_Area',
 'Loan_Status']
numerical_data = [i for i in data.columns if data[i].dtype!="object"]
numerical_data
['ApplicantIncome',
 'CoapplicantIncome',
 'LoanAmount',
 'Loan_Amount_Term',
 'Credit_History']
discrete_numerical_data = [i for i in numerical_data if len(data[i].unique())<16]
discrete_numerical_data
['Loan_Amount_Term', 'Credit_History']
continuous_numerical_data = [i for i in numerical_data if len(data[i].unique())>=16]
continuous_numerical_data
['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']
 
Data Visualisation
# For Categotical Data.
for i in categorical_data:
    data[i].value_counts().plot(kind="bar")
    plt.xlabel(i)
    plt.ylabel("Counts")
    plt.show()







# For Discrete Numerical Data
for i in discrete_numerical_data:
    data[i].value_counts().plot(kind="bar")
    plt.xlabel(i)
    plt.ylabel("Counts")
    plt.show()


# For Continuous Numerical Data
for i in continuous_numerical_data:
    sns.histplot(data[i])
    plt.xlabel(i+" Distribution")
    plt.show()



# Check for outliers(Since the above distribution graphs are skewed, hence outliers are present
for i in continuous_numerical_data:
    sns.boxplot(data = data, y=i)
    plt.show()



# From the above box plot we can know that outliers are present, hence we need to handle missing values by replacing with median value.
 
Handling Missing Values
sns.heatmap(data.isnull(), cbar=False)
<AxesSubplot:>

# Since there are null values in categorical values and discrete numerical values, so we replace them with mode of that feature.
for i in categorical_data+discrete_numerical_data:
    data[i] = data[i].fillna(data[i].mode().iloc[0])
# Now We replace the loan amount column from numerical category.
data["LoanAmount"] = data["LoanAmount"].fillna(data["LoanAmount"].median())
sns.heatmap(data.isnull(), cbar=False)
<AxesSubplot:>

# Correlation Matrix
data.corr()
ApplicantIncome	CoapplicantIncome	LoanAmount	Loan_Amount_Term	Credit_History
ApplicantIncome	1.000000	-0.116605	0.565181	-0.046531	-0.018615
CoapplicantIncome	-0.116605	1.000000	0.189218	-0.059383	0.011134
LoanAmount	0.565181	0.189218	1.000000	0.036960	-0.000607
Loan_Amount_Term	-0.046531	-0.059383	0.036960	1.000000	-0.004705
Credit_History	-0.018615	0.011134	-0.000607	-0.004705	1.000000
sns.heatmap(data.corr())
<AxesSubplot:>

 
Feature Engineering
# We will encode the categorical data using label Encoder.
le = preprocessing.LabelEncoder()

for i in categorical_data:
    data[i] = le.fit_transform(data[i])
data.head()
Gender	Married	Dependents	Education	Self_Employed	ApplicantIncome	CoapplicantIncome	LoanAmount	Loan_Amount_Term	Credit_History	Property_Area	Loan_Status
0	1	0	0	0	0	5849	0.0	128.0	360.0	1.0	2	1
1	1	1	1	0	0	4583	1508.0	128.0	360.0	1.0	0	0
2	1	1	0	0	1	3000	0.0	66.0	360.0	1.0	2	1
3	1	1	0	1	0	2583	2358.0	120.0	360.0	1.0	2	1
4	1	0	0	0	0	6000	0.0	141.0	360.0	1.0	2	1
# If we want to apply log transformation for all the numerical variables, then majority of CoapplicantIncome values are 0.
# Hence we will create a new variable called TotalIncome = ApplicantIncome + CoapplicantIncome.

data["TotalIncome"] = data["ApplicantIncome"]+data["CoapplicantIncome"]
data.drop(["ApplicantIncome","CoapplicantIncome"],axis=1,inplace=True)
continuous_numerical_data
['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']
continuous_numerical_data.remove("ApplicantIncome")
continuous_numerical_data.remove("CoapplicantIncome")
continuous_numerical_data.append("TotalIncome")
continuous_numerical_data
['LoanAmount', 'TotalIncome']
 
# Log Transformation
for i in continuous_numerical_data+["Loan_Amount_Term"]:
    data[i] = np.log(data[i])
data.head()
Gender	Married	Dependents	Education	Self_Employed	LoanAmount	Loan_Amount_Term	Credit_History	Property_Area	Loan_Status	TotalIncome
0	1	0	0	0	0	4.852030	5.886104	1.0	2	1	8.674026
1	1	1	1	0	0	4.852030	5.886104	1.0	0	0	8.714568
2	1	1	0	0	1	4.189655	5.886104	1.0	2	1	8.006368
3	1	1	0	1	0	4.787492	5.886104	1.0	2	1	8.505323
4	1	0	0	0	0	4.948760	5.886104	1.0	2	1	8.699515
 
Model Building
# Splitting the data.
X,y = data.drop(columns = "Loan_Status"),data["Loan_Status"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)
 
# Logistic Regression
model = LogisticRegression()
model.fit(X_train,y_train)
LogisticRegression()
print("Accuracy of Logistic Regression Model is ",model.score(X_test,y_test)*100)
Accuracy of Logistic Regression Model is  79.80295566502463
score = cross_val_score(model, X, y, cv=5)
print("Cross validation is",np.mean(score)*100)
Cross validation is 80.9462881514061
 
# import pickle
# # open a file, where you want to store the data
# file = open('model.pkl', 'wb')

# # dump information to that file
# pickle.dump(model, file)
 
